    
# 初始化 W 和 b
def initialize_parameters(layer_sizes, X):  #[5,4,1]
    weights = []
    biases = []
    weights.append(np.random.randn(layer_sizes[0], X.shape[1]))
    biases.append(np.random.randn(layer_sizes[0], 1))
    for i in range(1, len(layer_sizes)): #[0,1,2]
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i-1]))
        biases.append(np.zeros((layer_sizes[i], 1)))
    return weights, biases

# train NN
def train(X, y, layer_sizes, num_epochs=10000, learning_rate=0.01):
    input_size = X.shape[1]
    output_size = 1  # binary classification，输出层节点数为 1
    
    # 定义层大小
    #layer_sizes = [input_size] + hidden_layers + [output_size]
    
    # 初始化权重和偏置
    weights, biases = initialize_parameters(layer_sizes, X)
    
    for epoch in range(num_epochs):
        # 前向传播
        a_values, z_values = forward_propagation(X, weights, biases)
        
        # 计算损失（交叉熵损失）
        output = a_values[-1]
        loss = -np.mean(y * np.log(output) + (1 - y) * np.log(1 - output))
        
        # 反向传播
        backward_propagation(a_values, z_values, weights, biases, y, learning_rate)
        
        if epoch % 500 == 0:
            print(f'Epoch {epoch}, Loss: {loss:.4f}')
    
    return weights, biases

# 预测函数
def predict(X, weights, biases):
    a_values, _ = forward_propagation(X, weights, biases)
    output = a_values[-1]
    predictions = (output > 0.5).astype(int)
    return predictions

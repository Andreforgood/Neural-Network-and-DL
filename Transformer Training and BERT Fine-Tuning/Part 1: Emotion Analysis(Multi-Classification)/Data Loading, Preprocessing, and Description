import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# 下载NLTK所需资源（只需运行一次）
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

import pandas as pd
import torch
import torchvision
from sklearn.model_selection import train_test_split

# 读取CSV（请根据实际文件路径和列名进行修改）
emotion_df = pd.read_csv("/Users/dongwenou/Downloads/Deep Learning files/sentiment analysis/combined_emotion.csv")

# 查看前几行
print(emotion_df.head())

# 删除空值、重复值等（视数据情况而定）
emotion_df.dropna(inplace=True)
emotion_df.drop_duplicates(inplace=True)

# 映射情绪到数字标签
emotion_labels = {"joy": 0, "sad": 1, "anger": 2, "fear": 3, "love": 4, "suprise": 5}
emotion_df["label"] = emotion_df["emotion"].map(emotion_labels)

# 划分训练集和验证集（可根据自己需求，分出测试集）
# 文本预处理函数
def preprocess_text(text):
    # 1. 去除特殊字符和数字
    text = re.sub(r"[^a-zA-Z]", " ", text)
    # 2. 转换为小写
    text = text.lower()
    # 3. 分词
    words = text.split()
    # 4. 去除停用词
    stop_words = set(stopwords.words("english"))
    words = [w for w in words if not w in stop_words]
    # 5. 词形还原
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(w) for w in words]
    # 6. 将词语列表转换回字符串
    return " ".join(words)

# 应用文本预处理
emotion_df["sentence"] = emotion_df["sentence"].apply(preprocess_text)

# 划分训练集和验证集
train_df, val_df = train_test_split(emotion_df, test_size=0.1, random_state=369, stratify=emotion_df["label"])

print("Train size:", len(train_df))
print("Val size:", len(val_df))

print(train_df.head()) #查看预处理后的数据

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re

# Emotion 分布的可视化
emotion_counts = emotion_df["emotion"].value_counts()

# 预处理文本函数
def preprocess_text1(text):
    text = re.sub(r"[^a-zA-Z]", " ", text)
    text = text.lower()
    words = text.split()
    return words

# 新增一列统计词数
emotion_df["word_list"] = emotion_df["sentence"].apply(preprocess_text1)
emotion_df["word_count"] = emotion_df["word_list"].apply(len)

# 统计所有单词频率
all_words = [word for word_list in emotion_df["word_list"] for word in word_list]  # 展平所有分词
word_counts = Counter(all_words)
most_common_10 = word_counts.most_common(10)

# 创建子图
fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 三行一列

# 图 1: 情绪分布柱状图
sns.barplot(ax=axes[0], x=emotion_counts.index, y=emotion_counts.values)
axes[0].set_title("Emotion Distribution")
axes[0].set_xlabel("Emotion")
axes[0].set_ylabel("Count")

# 图 2: 句子长度分布直方图
sns.histplot(ax=axes[1], data=emotion_df["word_count"], bins=30, kde=True)
axes[1].set_title("Distribution of Word Counts per Sentence")
axes[1].set_xlabel("Word Count")
axes[1].set_ylabel("Frequency")

# 图 3: 最常见单词的柱状图
words, counts = zip(*most_common_10)
sns.barplot(ax=axes[2], x=list(counts), y=list(words), orient="h")
axes[2].set_title("Top 10 Most Common Words")
axes[2].set_xlabel("Frequency")
axes[2].set_ylabel("Words")

# 调整布局并保存
plt.tight_layout()
plt.savefig("report_visualization.png")  # 可选：保存为图像文件
plt.show()
